{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:48:46.995468Z","iopub.status.busy":"2023-10-17T18:48:46.995167Z","iopub.status.idle":"2023-10-17T18:48:49.783407Z","shell.execute_reply":"2023-10-17T18:48:49.782714Z","shell.execute_reply.started":"2023-10-17T18:48:46.995443Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import os\n","import re\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","\n","from matplotlib import pyplot as plt\n","\n","from xml.etree import ElementTree as et"]},{"cell_type":"markdown","metadata":{},"source":["# Maize Tassel dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:48:49.784993Z","iopub.status.busy":"2023-10-17T18:48:49.784770Z","iopub.status.idle":"2023-10-17T18:48:49.802859Z","shell.execute_reply":"2023-10-17T18:48:49.802190Z","shell.execute_reply.started":"2023-10-17T18:48:49.784972Z"},"trusted":true},"outputs":[],"source":["# defining the files directory and testing directory\n","files_dir = '../input/moda-foka-mtc/trainval'\n","test_dir = '../input/moda-foka-mtc/test'\n","\n","\n","class MaizeTasselDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, files_dir, width, height, transforms=None):\n","        self.transforms = transforms\n","        self.files_dir = files_dir\n","        self.height = height\n","        self.width = width\n","        \n","        # sorting the images for consistency\n","        # To get images, the extension of the filename is checked to be jpg\n","        self.imgs = [image for image in sorted(os.listdir(files_dir))\n","                        if image[-4:]=='.jpg']\n","        \n","        # classes: 0 index is reserved for background\n","        self.classes = [_, 'Maize']\n","\n","    def __getitem__(self, idx):\n","\n","        img_name = self.imgs[idx]\n","        image_path = os.path.join(self.files_dir, img_name)\n","\n","        # reading the images and converting them to correct size and color    \n","        img = cv2.imread(image_path)\n","        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n","        # diving by 255\n","        img_res /= 255.0\n","        \n","        # annotation file\n","        annot_filename = img_name[:-4] + '.xml'\n","        annot_file_path = os.path.join(self.files_dir, annot_filename)\n","        \n","        boxes = []\n","        labels = []\n","        tree = et.parse(annot_file_path)\n","        root = tree.getroot()\n","        \n","        # cv2 image gives size as height x width\n","        wt = img.shape[1]\n","        ht = img.shape[0]\n","        \n","        # box coordinates for xml files are extracted and corrected for image size given\n","        for member in root.findall('object'):\n","            labels.append(self.classes.index(member.find('name').text))\n","            \n","            # bounding box\n","            xmin = int(member.find('bndbox').find('xmin').text)\n","            xmax = int(member.find('bndbox').find('xmax').text)\n","            \n","            ymin = int(member.find('bndbox').find('ymin').text)\n","            ymax = int(member.find('bndbox').find('ymax').text)\n","            \n","            \n","            xmin_corr = (xmin/wt)*self.width\n","            xmax_corr = (xmax/wt)*self.width\n","            ymin_corr = (ymin/ht)*self.height\n","            ymax_corr = (ymax/ht)*self.height\n","            \n","            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n","        \n","        # convert boxes into a torch.Tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        \n","        # getting the areas of the boxes\n","        if len(boxes) !=0:\n","            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        else:\n","            area = torch.zeros((1,), dtype=torch.float32)\n","\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n","        \n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","        # image_id\n","        image_id = torch.tensor([idx])\n","        target[\"image_id\"] = image_id\n","\n","\n","        if self.transforms:\n","            \n","            sample = self.transforms(image = img_res,\n","                                     bboxes = target['boxes'],\n","                                     labels = labels)\n","            \n","            img_res = sample['image']\n","            target['boxes'] = torch.Tensor(sample['bboxes'])\n","            \n","            \n","            \n","        return img_res, target, img_name\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"markdown","metadata":{},"source":["# Data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:48:49.804360Z","iopub.status.busy":"2023-10-17T18:48:49.804070Z","iopub.status.idle":"2023-10-17T18:48:49.817808Z","shell.execute_reply":"2023-10-17T18:48:49.817072Z","shell.execute_reply.started":"2023-10-17T18:48:49.804300Z"},"trusted":true},"outputs":[],"source":["def get_transform(train):\n","    \n","    if train:\n","        return A.Compose([\n","                            A.OneOf([\n","                                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n","                                                     val_shift_limit=0.2, p=0.9),\n","                                A.RandomBrightnessContrast(brightness_limit=0.2, \n","                                                           contrast_limit=0.2, p=0.9),\n","                            ],p=0.9),\n","                            A.ToGray(p=0.01),\n","                            A.HorizontalFlip(p=0.5),\n","                            A.VerticalFlip(p=0.5),\n","                     # ToTensorV2 converts image to pytorch tensor without div by 255\n","                            ToTensorV2(p=1.0) \n","                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","    else:\n","        return A.Compose([\n","                            ToTensorV2(p=1.0)\n","                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"]},{"cell_type":"markdown","metadata":{},"source":["# TrainLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:48:49.820770Z","iopub.status.busy":"2023-10-17T18:48:49.820529Z","iopub.status.idle":"2023-10-17T18:48:50.009349Z","shell.execute_reply":"2023-10-17T18:48:50.008665Z","shell.execute_reply.started":"2023-10-17T18:48:49.820749Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_dataset = MaizeTasselDataset(files_dir, 1024, 1024, transforms= get_transform(train=True))\n","valid_dataset = MaizeTasselDataset(files_dir, 1024, 1024, transforms= get_transform(train=False))\n","\n","# split the dataset in train and test set\n","indices = torch.randperm(len(train_dataset)).tolist()\n","\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size= 16,\n","    shuffle=False,\n","    num_workers=4,\n","    collate_fn=collate_fn\n",")\n","\n","valid_data_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=4,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# FASTER RCNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:48:50.012115Z","iopub.status.busy":"2023-10-17T18:48:50.011869Z","iopub.status.idle":"2023-10-17T18:48:50.096884Z","shell.execute_reply":"2023-10-17T18:48:50.096144Z","shell.execute_reply.started":"2023-10-17T18:48:50.012093Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:13.447968Z","iopub.status.busy":"2023-10-17T18:49:13.447763Z","iopub.status.idle":"2023-10-17T18:49:14.996708Z","shell.execute_reply":"2023-10-17T18:49:14.996090Z","shell.execute_reply.started":"2023-10-17T18:49:13.447948Z"},"trusted":true},"outputs":[],"source":["# load a model; pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.001339Z","iopub.status.busy":"2023-10-17T18:49:14.999965Z","iopub.status.idle":"2023-10-17T18:49:15.006087Z","shell.execute_reply":"2023-10-17T18:49:15.005482Z","shell.execute_reply.started":"2023-10-17T18:49:15.001293Z"},"trusted":true},"outputs":[],"source":["num_classes = 2  # 1 class (tassel) + background\n","\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["# Some util functions"]},{"cell_type":"markdown","metadata":{},"source":["## Plot train history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.010627Z","iopub.status.busy":"2023-10-17T18:49:15.009090Z","iopub.status.idle":"2023-10-17T18:49:15.018123Z","shell.execute_reply":"2023-10-17T18:49:15.017396Z","shell.execute_reply.started":"2023-10-17T18:49:15.010598Z"},"trusted":true},"outputs":[],"source":["class Averager:\n","    def __init__(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","    def send(self, value):\n","        self.current_total += value\n","        self.iterations += 1\n","\n","    @property\n","    def value(self):\n","        if self.iterations == 0:\n","            return 0\n","        else:\n","            return 1.0 * self.current_total / self.iterations\n","\n","    def reset(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0"]},{"cell_type":"markdown","metadata":{},"source":["## Calculate intersect over union (IoU)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.019590Z","iopub.status.busy":"2023-10-17T18:49:15.019176Z","iopub.status.idle":"2023-10-17T18:49:15.028209Z","shell.execute_reply":"2023-10-17T18:49:15.027637Z","shell.execute_reply.started":"2023-10-17T18:49:15.019565Z"},"trusted":true},"outputs":[],"source":["def calculate_iou(gt, pr, form='pascal_voc') -> float:\n","    \"\"\"Calculates the Intersection over Union.\n","\n","    Args:\n","        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n","        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n","        form: (str) gt/pred coordinates format\n","            - pascal_voc: [xmin, ymin, xmax, ymax]\n","            - coco: [xmin, ymin, w, h]\n","    Returns:\n","        (float) Intersection over union (0.0 <= iou <= 1.0)\n","    \"\"\"\n","    if form == 'coco':\n","        gt = gt.copy()\n","        pr = pr.copy()\n","\n","        gt[2] = gt[0] + gt[2]\n","        gt[3] = gt[1] + gt[3]\n","        pr[2] = pr[0] + pr[2]\n","        pr[3] = pr[1] + pr[3]\n","\n","    # Calculate overlap area\n","    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n","    \n","    if dx < 0:\n","        return 0.0\n","    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n","\n","    if dy < 0:\n","        return 0.0\n","\n","    overlap_area = dx * dy\n","\n","    # Calculate union area\n","    union_area = (\n","            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n","            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n","            overlap_area\n","    )\n","\n","    return overlap_area / union_area"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.029667Z","iopub.status.busy":"2023-10-17T18:49:15.029206Z","iopub.status.idle":"2023-10-17T18:49:15.042693Z","shell.execute_reply":"2023-10-17T18:49:15.042071Z","shell.execute_reply.started":"2023-10-17T18:49:15.029643Z"},"trusted":true},"outputs":[],"source":["def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n","    \"\"\"Returns the index of the 'best match' between the\n","    ground-truth boxes and the prediction. The 'best match'\n","    is the highest IoU. (0.0 IoUs are ignored).\n","\n","    Args:\n","        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n","        pred: (List[Union[int, float]]) Coordinates of the predicted box\n","        pred_idx: (int) Index of the current predicted box\n","        threshold: (float) Threshold\n","        form: (str) Format of the coordinates\n","        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n","\n","    Return:\n","        (int) Index of the best match GT box (-1 if no match above threshold)\n","    \"\"\"\n","    best_match_iou = -np.inf\n","    best_match_idx = -1\n","    for gt_idx in range(len(gts)):\n","        \n","        if gts[gt_idx][0] < 0:\n","            # Already matched GT-box\n","            continue\n","        \n","        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n","\n","        if iou < 0:\n","            iou = calculate_iou(gts[gt_idx], pred, form=form)\n","            \n","            if ious is not None:\n","                ious[gt_idx][pred_idx] = iou\n","\n","        if iou < threshold:\n","            continue\n","\n","        if iou > best_match_iou:\n","            best_match_iou = iou\n","            best_match_idx = gt_idx\n","\n","    return best_match_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.044244Z","iopub.status.busy":"2023-10-17T18:49:15.043826Z","iopub.status.idle":"2023-10-17T18:49:15.057196Z","shell.execute_reply":"2023-10-17T18:49:15.056548Z","shell.execute_reply.started":"2023-10-17T18:49:15.044218Z"},"trusted":true},"outputs":[],"source":["def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n","    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n","\n","    Args:\n","        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n","        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n","               sorted by confidence value (descending)\n","        threshold: (float) Threshold\n","        form: (str) Format of the coordinates\n","        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n","\n","    Return:\n","        (float) Precision\n","    \"\"\"\n","    n = len(preds)\n","    tp = 0\n","    fp = 0\n","    \n","    for pred_idx in range(n):\n","\n","        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n","                                            threshold=threshold, form=form, ious=ious)\n","\n","        if best_match_gt_idx >= 0:\n","            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n","            tp += 1\n","            # Remove the matched GT box\n","            gts[best_match_gt_idx] = -1\n","        else:\n","            # No match\n","            # False positive: indicates a predicted box had no associated gt box.\n","            fp += 1\n","\n","    # False negative: indicates a gt box had no associated predicted box.\n","    fn = (gts.sum(axis=1) > 0).sum()\n","\n","    return tp / (tp + fp + fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.058730Z","iopub.status.busy":"2023-10-17T18:49:15.058240Z","iopub.status.idle":"2023-10-17T18:49:15.071813Z","shell.execute_reply":"2023-10-17T18:49:15.071001Z","shell.execute_reply.started":"2023-10-17T18:49:15.058687Z"},"trusted":true},"outputs":[],"source":["def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n","    \"\"\"Calculates image precision.\n","\n","    Args:\n","        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n","        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n","               sorted by confidence value (descending)\n","        thresholds: (float) Different thresholds\n","        form: (str) Format of the coordinates\n","\n","    Return:\n","        (float) Precision\n","    \"\"\"\n","    n_threshold = len(thresholds)\n","    image_precision = 0.0\n","    \n","    ious = np.ones((len(gts), len(preds))) * -1\n","    # ious = None\n","\n","    for threshold in thresholds:\n","        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n","                                                     form=form, ious=ious)\n","        image_precision += precision_at_threshold / n_threshold\n","\n","    return image_precision"]},{"cell_type":"markdown","metadata":{},"source":["# Model Train AND VALIDATION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.075245Z","iopub.status.busy":"2023-10-17T18:49:15.075000Z","iopub.status.idle":"2023-10-17T18:49:15.144159Z","shell.execute_reply":"2023-10-17T18:49:15.143510Z","shell.execute_reply.started":"2023-10-17T18:49:15.075223Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","num_epochs = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T18:49:15.148730Z","iopub.status.busy":"2023-10-17T18:49:15.147300Z","iopub.status.idle":"2023-10-17T19:01:50.354414Z","shell.execute_reply":"2023-10-17T19:01:50.353654Z","shell.execute_reply.started":"2023-10-17T18:49:15.148700Z"},"trusted":true},"outputs":[],"source":["train_hist = Averager()\n","t = 1\n","valid_pred_min = 0.65 \n","for epoch in range(num_epochs):\n","    train_hist.reset()\n","    \n","    model.train()\n","    for images, targets, image_ids in train_data_loader:\n","        \n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","\n","        losses = sum(loss for loss in loss_dict.values())\n","        train_loss = losses.item()\n","\n","        train_hist.send(train_loss)\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if t % 50 == 0:\n","            print(f\"Iteration #{t} loss: {train_loss}\")\n","\n","        t += 1\n","    \n","    \n","    model.eval()\n","    validation_image_precisions = []\n","    iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n","    for images, targets, image_ids in valid_data_loader:       \n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        \n","        with torch.no_grad():\n","            outputs = model(images)\n","            \n","        \n","        \n","        \n","        for i, image in enumerate(images):\n","            boxes = outputs[i]['boxes'].data.cpu().numpy()\n","            scores = outputs[i]['scores'].data.cpu().numpy()\n","            gt_boxes = targets[i]['boxes'].cpu().numpy()\n","            preds_sorted_idx = np.argsort(scores)[::-1]\n","            preds_sorted = boxes[preds_sorted_idx]\n","            image_precision = calculate_image_precision(preds_sorted,\n","                                                        gt_boxes,\n","                                                        thresholds=iou_thresholds,\n","                                                        form='coco')\n","            validation_image_precisions.append(image_precision)\n","\n","    valid_prec = np.mean(validation_image_precisions)\n","    print(\"Validation IOU: {0:.4f}\".format(valid_prec))\n","              \n","       \n","    \n","    #print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","        epoch, \n","        train_loss\n","    ))\n","        \n","    ## TODO: save the model if validation precision has decreased\n","    if valid_prec >= valid_pred_min:\n","        print('Validation precision increased({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_pred_min,\n","            valid_prec))\n","        torch.save(model.state_dict(), 'fasterrcnn.pth')\n","        valid_pred_min = valid_prec\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Sample"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T19:01:50.355678Z","iopub.status.busy":"2023-10-17T19:01:50.355436Z","iopub.status.idle":"2023-10-17T19:02:07.112076Z","shell.execute_reply":"2023-10-17T19:02:07.111187Z","shell.execute_reply.started":"2023-10-17T19:01:50.355653Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","images, targets, image_ids = next(iter(valid_data_loader))\n","images = list(image.to(device) for image in images)\n","outputs = model(images)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T19:02:07.113515Z","iopub.status.busy":"2023-10-17T19:02:07.113267Z","iopub.status.idle":"2023-10-17T19:02:07.127378Z","shell.execute_reply":"2023-10-17T19:02:07.126802Z","shell.execute_reply.started":"2023-10-17T19:02:07.113490Z"},"trusted":true},"outputs":[],"source":["detection_threshold = 0.6\n","sample = images[1].permute(1,2,0).cpu().numpy()\n","boxes = outputs[1]['boxes'].data.cpu().numpy()\n","scores = outputs[1]['scores'].data.cpu().numpy()\n","\n","boxes = boxes[scores >= detection_threshold].astype(np.int32)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T19:02:07.128830Z","iopub.status.busy":"2023-10-17T19:02:07.128616Z","iopub.status.idle":"2023-10-17T19:02:07.444295Z","shell.execute_reply":"2023-10-17T19:02:07.443217Z","shell.execute_reply.started":"2023-10-17T19:02:07.128810Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","\n","for box in boxes:\n","    cv2.rectangle(sample,\n","                  (box[0], box[1]),\n","                  (box[2], box[3]),\n","                  (220, 0, 0), 2)\n","    \n","ax.set_axis_off()\n","ax.imshow(sample)"]},{"cell_type":"markdown","metadata":{},"source":["## Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T19:02:07.445749Z","iopub.status.busy":"2023-10-17T19:02:07.445423Z","iopub.status.idle":"2023-10-17T19:02:07.714192Z","shell.execute_reply":"2023-10-17T19:02:07.713544Z","shell.execute_reply.started":"2023-10-17T19:02:07.445725Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'fasterrcnn_1.pth')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
